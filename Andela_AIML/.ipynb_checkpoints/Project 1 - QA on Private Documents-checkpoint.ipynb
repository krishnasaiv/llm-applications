{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "081be0e7",
   "metadata": {},
   "source": [
    "## Build Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02abb73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pinecone\n",
    "\n",
    "#vector stores\n",
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "#Embeddings\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "\n",
    "# Document Loaders\n",
    "from langchain.document_loaders import PyPDFLoader, Docx2txtLoader, TextLoader, UnstructuredMarkdownLoader, WikipediaLoader\n",
    "\n",
    "# Text Splitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# LLMs, Memory & Chains\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import HuggingFaceHub\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains import ConversationalRetrievalChain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab07e80",
   "metadata": {},
   "source": [
    "### 1. Load env files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c15685b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1762f334",
   "metadata": {},
   "source": [
    "### 2. Load Documents ( from a single file or from a directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e6e2f4",
   "metadata": {},
   "source": [
    "#### 2.1 Load from File\n",
    "##### Supported formats : .pdf, .docx, .txt, .md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "880af8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document(file_path):\n",
    "    name, extension = os.path.splitext(file_path)\n",
    "    \n",
    "    if extension == '.pdf':\n",
    "        loader = PyPDFLoader(file_path)\n",
    "    elif extension == '.docx':\n",
    "        loader = Docx2txtLoader(file_path)\n",
    "    elif extension == '.txt':\n",
    "        loader = TextLoader(file_path)\n",
    "    elif extension == '.md':\n",
    "        loader = UnstructuredMarkdownLoader(file_path)\n",
    "    else:\n",
    "        print(f\"Unsupported File Format {file_path}. Supported formats: .pdf, .docx, .txt, .md\")\n",
    "        return []    \n",
    "        \n",
    "    print(f\"Reading {file_path}\")\n",
    "    return loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590f0914",
   "metadata": {},
   "source": [
    "#### 2.2 Load from a directory\n",
    "##### Loads all files from a directory. Supported formats : .pdf, .docx, .txt, .md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ac1e3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### function that either takes a file path & reads the text from the file \n",
    "##### or a folder path & reads the text from each file \n",
    "def load_from(path, nested=False):\n",
    "    ### If a file path is passed\n",
    "    if os.path.isfile(path):\n",
    "        return load_document(path)\n",
    "    \n",
    "    ### If a directory is passed\n",
    "    elif os.path.isdir(path):\n",
    "        print(f\"Reading from folder {path}\")\n",
    "        item_paths = [os.path.join(path, f) for f in os.listdir(path)]\n",
    "        \n",
    "        loaded_docs = []\n",
    "        for p in item_paths:\n",
    "#             print(p)\n",
    "            if os.path.isfile(p):\n",
    "                loaded_docs += load_document(p)\n",
    "            elif nested and os.path.isdir(p):\n",
    "                loaded_docs += load_from(p)\n",
    "        \n",
    "        return loaded_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0006b0",
   "metadata": {},
   "source": [
    "#### 2.2 Load from External sources: \n",
    "##### Supoprted Websites: Wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5aa435e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_wiki(query, load_max_docs=1, max_chars_per_doc=5000):\n",
    "    loader = WikipediaLoader(query=query, load_max_docs=load_max_docs, doc_content_chars_max=max_chars_per_doc)\n",
    "    data = loader.load()\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30a10fc",
   "metadata": {},
   "source": [
    "### 3. Make Chunks from Documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "388ba798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_data(data, chunk_size=400, chunk_overlap=80):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap )\n",
    "    chunks = text_splitter.split_documents(data)\n",
    "    return chunks\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e437588",
   "metadata": {},
   "source": [
    "### 4. Map chukns into embeddings & upload to Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8e1aa860",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_embedding_cost(texts):\n",
    "    import tiktoken\n",
    "    enc = tiktoken.encoding_for_model('text-embedding-ada-002')\n",
    "    total_tokens = sum([len(enc.encode(page.page_content)) for page in texts])\n",
    "    print(f'Total Tokens: {total_tokens}')\n",
    "    print(f'Embedding Cost in USD: {total_tokens / 1000 * 0.0004:.6f}')\n",
    "\n",
    "def insert_or_fetch_embeddings(index_name, chunks, embeddings_type='instruct'):\n",
    "    \n",
    "    if embeddings_type == 'instruct':\n",
    "        embeddings = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\", )\n",
    "        embedding_dimension = 768\n",
    "    elif embeddings_type == 'openai':\n",
    "        embeddings = OpenAIEmbeddings()\n",
    "        embedding_dimension = 1536\n",
    "    else:\n",
    "        print(\"Unknown Embeddings type.\")\n",
    "        return None\n",
    "        \n",
    "    # Inititate connection to the PineCone\n",
    "    pinecone.init(api_key=os.environ.get('PINECONE_API_KEY'), environment=os.environ.get('PINECONE_ENV'))\n",
    "\n",
    "    if index_name in pinecone.list_indexes():\n",
    "        print(f'Index {index_name} already exists. Loading embeddings ... ', end='')\n",
    "        vector_store = Pinecone.from_existing_index(\n",
    "                                index_name, \n",
    "                                embeddings)\n",
    "        print('Done')\n",
    "    else:\n",
    "        print(f'Creating index {index_name} and mapping chunks into embeddings ...', end='')\n",
    "        pinecone.create_index(index_name, \n",
    "                              dimension=embedding_dimension, \n",
    "                              metric='cosine')\n",
    "        vector_store = Pinecone.from_documents(\n",
    "                                chunks, \n",
    "                                embeddings, \n",
    "                                index_name=index_name)\n",
    "        print('Done')\n",
    "\n",
    "    return vector_store\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a95ac0",
   "metadata": {},
   "source": [
    "### 5. Build LLM Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52ecb134",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm_chain(vector_store, llm_type = 'google', has_memory=False,):\n",
    "    # 1. LLM\n",
    "    if llm_type == 'google':\n",
    "        llm = HuggingFaceHub(repo_id=\"google/flan-t5-xxl\", model_kwargs={\"temperature\":0.1}) # repo_id = 'meta-llama/Llama-2-70b-chat'\n",
    "    elif llm_type == 'openai':\n",
    "        llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0.1)\n",
    "    \n",
    "    # 2. Vector Store retriever\n",
    "    retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': 3})\n",
    "    \n",
    "    # 3. Define Chain\n",
    "    if not has_memory:\n",
    "        chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
    "    else:\n",
    "        llm_memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "        chain = ConversationalRetrievalChain.from_llm(llm, retriever, memory=llm_memory)\n",
    "        \n",
    "    return chain\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f6b6c9",
   "metadata": {},
   "source": [
    "### 6. Asking Questions & Getting Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02bdd524",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(query, llm_chain, has_memory=False, chat_history = []):\n",
    "    if not has_memory:\n",
    "        answer = llm_chain.run(q)\n",
    "        chat_history=[]\n",
    "    else:\n",
    "        chain_output = llm_chain({\"question\": query})\n",
    "        answer, chat_history = chain_output['answer'], chain_output['chat_history']\n",
    "    return answer, chat_history\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd135956",
   "metadata": {},
   "source": [
    "# Main Program Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0eb643",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d7b477",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce62a69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f9429a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------- \n",
      "--                                                                  -- \n",
      "--            Welcome to your private chat application. !           -- \n",
      "--                                                                  --\n",
      "----------------------------------------------------------------------\n",
      "Before you start using your private chatbot, please complete Phase 1 & Phase 2 setiup.!\n",
      "------- Phase 1: Configure your chat agent \n",
      "\n",
      "|***| Select the LLM you want to use? Enter the corrresponding number or Press enter to use the deault one\n",
      "\t1. Google/flan-t5-xxl (default)\n",
      "\t2. OpenAI\n",
      "2\n",
      "\tSelected OpenAI\n",
      "\n",
      "|***| Select the embeddings you want to use? Enter the corrresponding number or Press enter to use the deault one\n",
      "\t1. instructor-xl (default)\n",
      "\t2. OpenAI\n",
      "2\n",
      "\tSelected OpenAI\n",
      "\n",
      "|***| Do you want your agent to have memory and remember your older conversations?\n",
      "\t1. No (default)\n",
      "\t2. Yes\n",
      "2\n",
      "\tSelected Yes\n",
      "------- Phase 1: Configuration complete -------\n",
      "\n",
      "\n",
      "------- Phase 2: Build Knowledge Base -------\n",
      "\n",
      "|***| Select where should the LLM build the knowledge base from? Enter the corrresponding number or Press enter to use the deault one\n",
      "\t1. Local files (Supported formats: '.pdf', '.txt', '.md' (default)\n",
      "\t2. Web (Wikipedia)\n",
      "2\n",
      "\tSelected Web (Wikipedia)\n",
      "\n",
      "Select the topic you want to search.Harry Potter\n"
     ]
    }
   ],
   "source": [
    "# def main():\n",
    "print(\"-\"*70,\"\\n--\",\" \"*64, \"--\",\"\\n--\",\" \"*9,\" Welcome to your private chat application. !\",\" \"*9, \"--\", \"\\n--\",\" \"*64, \"--\\n\"+\"-\"*70)\n",
    "print(\"Before you start using your private chatbot, please complete Phase 1 & Phase 2 setiup.!\")\n",
    "print(\"------- Phase 1: Configure your chat agent \")\n",
    "print(\"\\n|***| Select the LLM you want to use? Enter the corrresponding number or Press enter to use the deault one\")\n",
    "print(\"\\t1. Google/flan-t5-xxl (default)\\n\\t2. OpenAI\")\n",
    "llm_choice = input()\n",
    "llm_type = 'openai' if llm_choice == '2' else 'google'\n",
    "print(f\"\\tSelected {'OpenAI' if llm_choice=='2' else 'Google/flan-t5-xxl (default)'}\")\n",
    "\n",
    "print(\"\\n|***| Select the embeddings you want to use? Enter the corrresponding number or Press enter to use the deault one\")\n",
    "print(\"\\t1. instructor-xl (default)\\n\\t2. OpenAI\")\n",
    "embedding_choice = input()\n",
    "embeddings_type = 'openai' if embedding_choice=='2' else 'instruct'\n",
    "print(f\"\\tSelected {'OpenAI' if embedding_choice=='2' else 'instructor-xl (default)'}\")\n",
    "\n",
    "print(\"\\n|***| Do you want your agent to have memory and remember your older conversations?\")\n",
    "print(\"\\t1. No (default)\\n\\t2. Yes\")\n",
    "memory_choice = input()\n",
    "has_memory = True if memory_choice=='2' else False\n",
    "print(f\"\\tSelected {'Yes' if memory_choice=='2' else 'No (default)'}\")\n",
    "print(\"------- Phase 1: Configuration complete -------\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"------- Phase 2: Build Knowledge Base -------\")\n",
    "print(\"\\n|***| Select where should the LLM build the knowledge base from? Enter the corrresponding number or Press enter to use the deault one\")\n",
    "print(\"\\t1. Local files (Supported formats: '.pdf', '.txt', '.md' (default)\\n\\t2. Web (Wikipedia)\")\n",
    "input_choice = input()\n",
    "print(f\"\\tSelected {'Web (Wikipedia)' if input_choice=='2' else 'Local files'}\\n\")\n",
    "if input_choice == '2':\n",
    "    query=input(\"Select the topic you want to search.\")\n",
    "    docs = load_from_wiki(query)\n",
    "else:\n",
    "    input_path = input(\"Enter the path (file/ folder) to read documents from. (Supported formats: '.pdf', '.txt', '.md')\")\n",
    "    docs = load_from(input_path)\n",
    "    \n",
    "\n",
    "print(\"\\n---- Chunking the Data ... \", end='')\n",
    "chunks = chunk_data(docs)\n",
    "print(\"Done\\n\")\n",
    "\n",
    "index_name = input(\"|***| Enter the name of name of Pinecone index that you want to create or fetch from.\")\n",
    "print(\"---- Upload Chunks to Vector Store\\n\")\n",
    "if embeddings_type == 'openai': print(\"Estimated cost:\", print_embedding_cost(chunks))\n",
    "vector_store = insert_or_fetch_embeddings(index_name= index_name, chunks= chunks, embeddings_type=embeddings_type)\n",
    "print(\"------- Phase 2: Setting up knowledge based vector store complete -------\\n\")\n",
    "\n",
    "\n",
    "# 4. Build LLM Chain\n",
    "llmchain = get_llm_chain(vector_store=vector_store, llm_type = llm_type, has_memory=has_memory,)\n",
    "\n",
    "#5. Asking Questions & Getting Answers\n",
    "import time\n",
    "num = 1\n",
    "print(\"*\"*77)\n",
    "print('****************** Your private chatbot is ready for use! ******************')\n",
    "print(\"*\"*77+\"\\nEnter Q/ Quit/ Exit to quit.\")\n",
    "while True:\n",
    "    print(f'{\"-\" * 50} \\n')\n",
    "    q = input(f'Question #{num}: ')\n",
    "    num += 1\n",
    "    if q.lower() in ['q', 'quit', 'exit']:\n",
    "        print('Conversation ended...')\n",
    "        time.sleep(2)\n",
    "        break\n",
    "    \n",
    "    answer, chat_history = ask(query=q, llm_chain=llmchain, has_memory=has_memory, chat_history = [])\n",
    "    print(f'Answer: {answer}')\n",
    "    if has_memory:\n",
    "        print(f\"Chat History: {chat_history}\")\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7b9fb2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Mia Khalifa (; Arabic: ميا خليفة, romanized: Miyа̄ Ḵalīfah [mijaː χaliːfa(h)]; born 1993) is a Lebanese-American media personality and former pornographic film actress and webcam model. She began acting in pornography in October 2014, becoming the most viewed performer on Pornhub in two months. Her career choice was met with controversy in the Middle East, especially for a video in which she performed sexual acts while wearing a hijab.\\n\\n\\n== Early life and education ==\\nKhalifa was born in Beirut, Lebanon and raised Catholic in what she describes as a \"very conservative\" home, Khalifa attended a French-language private school in Beirut, where she also learned to speak English. She moved with her family to the United States in 2001, leaving their home in the wake of the South Lebanon conflict.After moving to the United States, her family lived in Montgomery County, Maryland, where she played lacrosse in high school. She has said she was bullied at school for being \"the darkest and weirdest girl there\", which intensified after the September 11 attacks.She attended Massanutten Military Academy in Woodstock, Virginia, and later graduated from the University of Texas at El Paso with a Bachelor of Arts in history. She supported herself while there by working as a bartender, model, and \"briefcase girl\" on a local Deal or No Deal-esque Spanish game show.\\n\\n\\n== Pornographic film career ==\\nKhalifa entered the professional pornographic film industry in October 2014. Her stage name was taken from the name of her dog, Mia, and American rapper Wiz Khalifa.She came to widespread attention after the release of a scene from Bang Bros in which she wears a hijab during a threesome. The scene brought Khalifa instant popularity, as well as criticism from writers and religious figures, and led to her parents publicly disowning her. Alex Hawkins, vice president of marketing for xHamster, said, \"The outrage it caused in the Arab world ended up being a bit of a \\'Streisand effect\\'. Suddenly, everyone was searching for her. The effort to censor her only made her more ubiquitous.\" With more than 1.5 million views, the 22-year-old Khalifa became the most searched-for performer on the adult video sharing website Pornhub.On December 28, 2018, Pornhub revealed that she was the No. 1 ranked performer on their website.\" After becoming the most searched-for actress on Pornhub, Khalifa received online death threats, including a manipulated image of Khalifa being held captive by an Islamic State executioner and a warning that she would be \"the first person in Hellfire\", to which she jokingly replied, \"I\\'ve been meaning to get a little tan recently.\" Lebanese newspapers wrote articles critical of Khalifa, which she considered trivial due to other events in the region.In an interview with The Washington Post, Khalifa said the controversial scene was satirical and should be taken as such, claiming that Hollywood films depicted Muslims in a far more negative light than any pornographer could. Among those who publicly spoke out to defend her decision to become an adult performer was British-Lebanese author Nasri Atallah, who stated, \"The moral indignation ... is wrong for two reasons. First and foremost, as a woman, she is free to do as she pleases with her body. As a sentient human being with agency, who lives halfway across the world, she is in charge of her own life and owes absolutely nothing to the country where she happened to be born.\" Khalifa said, \"Women\\'s rights in Lebanon are a long way from being taken seriously if a Lebanese American porn star that no longer resides there can cause such an uproar. What I once boasted to people as being the most Westernized nation in the Middle East, I now see as devastatingly archaic and oppressed.\"According to data from Pornhub, from January 3 to 6, 2015, searches for Khalifa increased five-fold. Around a quarter of those searches came from Lebanon, with substantial searches also from nearby countries Syria and Jordan. Almaza, a Lebanese brewery, ran an advertisement showing a bottle of their beer next to Khalifa\\'s signature glasses, with the slogan: \"We are both rated 18+.\" In January 2015, pop-rap duo Timeflies released a song titled \"Mia Khalifa\" in homage to her.In January 2015, Khalifa signed a long-term contract with Bang Bros\\' parent company, WGCZ Holding, who also own the largest free porn site XVideos. The contract required her to perform in multiple films each month. However, two weeks later, Khalifa had a change of heart and resigned. The negative attention she received from her global attention prompted her to leave the industry: \"It was an eye-opener for me. I don\\'t want any of this, whether it\\'s positive or negative—but all of it was negative. I didn\\'t think too much into it about how my friends and family and relationships were suffering.\" WGCZ Holding own a web page with a domain name using her stage name. Khalifa said it does not pay her for rights, even though it is written in her first', metadata={'title': 'Mia Khalifa', 'summary': 'Mia Khalifa (; Arabic: ميا خليفة, romanized: Miyа̄ Ḵalīfah [mijaː χaliːfa(h)]; born 1993) is a Lebanese-American media personality and former pornographic film actress and webcam model. She began acting in pornography in October 2014, becoming the most viewed performer on Pornhub in two months. Her career choice was met with controversy in the Middle East, especially for a video in which she performed sexual acts while wearing a hijab.', 'source': 'https://en.wikipedia.org/wiki/Mia_Khalifa'})]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_from_wiki(\"Mia Khalifa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17848eb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d = llmchain({\"question\": \"Who is harry\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84174d3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Who is harry', additional_kwargs={}, example=False),\n",
       " AIMessage(content='Harry Potter', additional_kwargs={}, example=False),\n",
       " HumanMessage(content='Who is harry', additional_kwargs={}, example=False),\n",
       " AIMessage(content='a wizard', additional_kwargs={}, example=False),\n",
       " HumanMessage(content='Who is harry', additional_kwargs={}, example=False),\n",
       " AIMessage(content='Harry is a boy.', additional_kwargs={}, example=False)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['chat_history']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ecc73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "harry-potter-part1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3fd3942b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "../../../Code/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
