{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "081be0e7",
   "metadata": {},
   "source": [
    "## Build Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02abb73d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/krishnasai/miniconda3/envs/llm/lib/python3.11/site-packages/pinecone/index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pinecone\n",
    "import tiktoken\n",
    "\n",
    "#vector stores\n",
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "#Embeddings\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "\n",
    "# Document Loaders\n",
    "from langchain.document_loaders import PyPDFLoader, Docx2txtLoader, TextLoader, UnstructuredMarkdownLoader, WikipediaLoader\n",
    "\n",
    "# Text Splitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# LLMs, Memory & Chains\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import HuggingFaceHub\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains import ConversationalRetrievalChain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab07e80",
   "metadata": {},
   "source": [
    "### 1. Load env files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c15685b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1762f334",
   "metadata": {},
   "source": [
    "### 2. Load Documents ( from a single file or from a directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e6e2f4",
   "metadata": {},
   "source": [
    "#### 2.1 Load from File\n",
    "##### Supported formats : .pdf, .docx, .txt, .md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "880af8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document(file_path):\n",
    "    name, extension = os.path.splitext(file_path)\n",
    "    \n",
    "    if extension == '.pdf':\n",
    "        loader = PyPDFLoader(file_path)\n",
    "    elif extension == '.docx':\n",
    "        loader = Docx2txtLoader(file_path)\n",
    "    elif extension == '.txt':\n",
    "        loader = TextLoader(file_path)\n",
    "    elif extension == '.md':\n",
    "        loader = UnstructuredMarkdownLoader(file_path)\n",
    "    else:\n",
    "        print(f\"Unsupported File Format {file_path}. Supported formats: .pdf, .docx, .txt, .md\")\n",
    "        return []    \n",
    "        \n",
    "    print(f\"Reading {file_path}\")\n",
    "    return loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590f0914",
   "metadata": {},
   "source": [
    "#### 2.2 Load from a directory\n",
    "##### Loads all files from a directory. Supported formats : .pdf, .docx, .txt, .md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ac1e3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### function that either takes a file path & reads the text from the file \n",
    "##### or a folder path & reads the text from each file \n",
    "def load_from(path, nested=False):\n",
    "    ### If a file path is passed\n",
    "    if os.path.isfile(path):\n",
    "        return load_document(path)\n",
    "    \n",
    "    ### If a directory is passed\n",
    "    elif os.path.isdir(path):\n",
    "        print(f\"Reading from folder {path}\")\n",
    "        item_paths = [os.path.join(path, f) for f in os.listdir(path)]\n",
    "        \n",
    "        loaded_docs = []\n",
    "        for p in item_paths:\n",
    "#             print(p)\n",
    "            if os.path.isfile(p):\n",
    "                loaded_docs += load_document(p)\n",
    "            elif nested and os.path.isdir(p):\n",
    "                loaded_docs += load_from(p)\n",
    "        \n",
    "        return loaded_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0006b0",
   "metadata": {},
   "source": [
    "#### 2.2 Load from External sources: \n",
    "##### Supoprted Websites: Wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5aa435e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_wiki(query, load_max_docs=1, max_chars_per_doc=5000):\n",
    "    loader = WikipediaLoader(query=query, load_max_docs=load_max_docs, doc_content_chars_max=max_chars_per_doc)\n",
    "    data = loader.load()\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30a10fc",
   "metadata": {},
   "source": [
    "### 3. Make Chunks from Documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "388ba798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_data(data, chunk_size=400, chunk_overlap=80):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap )\n",
    "    chunks = text_splitter.split_documents(data)\n",
    "    return chunks\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e437588",
   "metadata": {},
   "source": [
    "### 4. Map chukns into embeddings & upload to Pinecone\n",
    "![image](https://files.readme.io/6a3ea5a-pinecone-openai-overview.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e1aa860",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_or_fetch_embeddings(index_name, chunks, embeddings_type='instruct'):\n",
    "    \n",
    "    if embeddings_type == 'instruct':\n",
    "        embeddings = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\", )\n",
    "        embedding_dimension = 768\n",
    "    elif embeddings_type == 'openai':\n",
    "        embeddings = OpenAIEmbeddings()\n",
    "        embedding_dimension = 1536\n",
    "    else:\n",
    "        print(\"Unknown Embeddings type.\")\n",
    "        return None\n",
    "        \n",
    "    # Inititate connection to the PineCone\n",
    "    pinecone.init(api_key=os.environ.get('PINECONE_API_KEY'), environment=os.environ.get('PINECONE_ENV'))\n",
    "\n",
    "    if index_name in pinecone.list_indexes():\n",
    "        print(f'Index {index_name} already exists. Loading embeddings ... ', end='')\n",
    "        vector_store = Pinecone.from_existing_index(\n",
    "                                index_name, \n",
    "                                embeddings)\n",
    "        print('Done')\n",
    "    else:\n",
    "        print(f'Creating index {index_name} and mapping chunks into embeddings ...', end='')\n",
    "        pinecone.create_index(index_name, \n",
    "                              dimension=embedding_dimension, \n",
    "                              metric='cosine')\n",
    "        vector_store = Pinecone.from_documents(\n",
    "                                chunks, \n",
    "                                embeddings, \n",
    "                                index_name=index_name)\n",
    "        print('Done')\n",
    "\n",
    "    return vector_store\n",
    "        \n",
    "        \n",
    "\n",
    "def print_embedding_cost(texts):\n",
    "    \n",
    "    enc = tiktoken.encoding_for_model('text-embedding-ada-002')\n",
    "    total_tokens = sum([len(enc.encode(page.page_content)) for page in texts])\n",
    "    print(f'Total Tokens: {total_tokens}')\n",
    "    print(f'Embedding Cost in USD: {total_tokens / 1000 * 0.0004:.6f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a95ac0",
   "metadata": {},
   "source": [
    "### 5. Build LLM Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52ecb134",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm_chain(vector_store, llm_type = 'google', has_memory=False,):\n",
    "    # 1. LLM\n",
    "    if llm_type == 'google':\n",
    "        llm = HuggingFaceHub(repo_id=\"google/flan-t5-xxl\", model_kwargs={\"temperature\":0.1}) # repo_id = 'meta-llama/Llama-2-70b-chat'\n",
    "    elif llm_type == 'openai':\n",
    "        llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0.1)\n",
    "    elif llm_type == 'facebook':\n",
    "        llm = HuggingFaceHub(repo_id=\"meta-llama/Llama-2-70b-chat\") #, model_kwargs={\"temperature\":0.1}\n",
    "    \n",
    "    # 2. Vector Store retriever\n",
    "    retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': 5})\n",
    "    \n",
    "    # 3. Define Chain\n",
    "    if not has_memory:\n",
    "        chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
    "    else:\n",
    "        llm_memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "        chain = ConversationalRetrievalChain.from_llm(llm, retriever, memory=llm_memory)\n",
    "        \n",
    "    return chain\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f6b6c9",
   "metadata": {},
   "source": [
    "### 6. Asking Questions & Getting Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02bdd524",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(query, llm_chain, has_memory=False, chat_history = []):\n",
    "    if not has_memory:\n",
    "        answer = llm_chain.run(q)\n",
    "        chat_history=[]\n",
    "    else:\n",
    "        chain_output = llm_chain({\"question\": query})\n",
    "        answer, chat_history = chain_output['answer'], chain_output['chat_history']\n",
    "    return answer, chat_history\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd135956",
   "metadata": {},
   "source": [
    "## Main Program Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67f9429a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"-\"*70,\"\\n--\",\" \"*64, \"--\",\"\\n--\",\" \"*9,\" Welcome to your private chat application. !\",\" \"*9, \"--\", \"\\n--\",\" \"*64, \"--\\n\"+\"-\"*70)\n",
    "    print(\"Before you start using your private chatbot, please complete Phase 1 & Phase 2 setiup.!\")\n",
    "    print(\"------- Phase 1: Configure your chat agent \")\n",
    "    print(\"\\n|***| Select the LLM you want to use? Enter the corrresponding number or Press enter to use the deault one\")\n",
    "    print(\"\\t1. Google/flan-t5-xxl (default)\\n\\t2. OpenAI\")\n",
    "    llm_choice = input()\n",
    "    llm_type = 'openai' if llm_choice == '2' else 'google'\n",
    "    print(f\"\\tSelected {'OpenAI' if llm_choice=='2' else 'Google/flan-t5-xxl (default)'}\")\n",
    "\n",
    "    print(\"\\n|***| Select the embeddings you want to use? Enter the corrresponding number or Press enter to use the deault one\")\n",
    "    print(\"\\t1. instructor-xl (default)\\n\\t2. OpenAI\")\n",
    "    embedding_choice = input()\n",
    "    embeddings_type = 'openai' if embedding_choice=='2' else 'instruct'\n",
    "    print(f\"\\tSelected {'OpenAI' if embedding_choice=='2' else 'instructor-xl (default)'}\")\n",
    "\n",
    "    print(\"\\n|***| Do you want your agent to have memory and remember your older conversations?\")\n",
    "    print(\"\\t1. No (default)\\n\\t2. Yes\")\n",
    "    memory_choice = input()\n",
    "    has_memory = True if memory_choice=='2' else False\n",
    "    print(f\"\\tSelected {'Yes' if memory_choice=='2' else 'No (default)'}\")\n",
    "    print(\"------- Phase 1: Configuration complete -------\")\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"------- Phase 2: Build Knowledge Base -------\")\n",
    "    print(\"\\n|***| Select where should the LLM build the knowledge base from? Enter the corrresponding number or Press enter to use the deault one\")\n",
    "    print(\"\\t1. Local files (Supported formats: '.pdf', '.txt', '.md' (default)\\n\\t2. Web (Wikipedia)\")\n",
    "    input_choice = input()\n",
    "    print(f\"\\tSelected {'Web (Wikipedia)' if input_choice=='2' else 'Local files'}\\n\")\n",
    "    if input_choice == '2':\n",
    "        query=input(\"Select the topic you want to search.\")\n",
    "        docs = load_from_wiki(query)\n",
    "    else:\n",
    "        input_path = input(\"Enter the path (file/ folder) to read documents from. (Supported formats: '.pdf', '.txt', '.md')\")\n",
    "        docs = load_from(input_path)\n",
    "\n",
    "\n",
    "    print(\"\\n---- Chunking the Data ... \", end='')\n",
    "    chunks = chunk_data(docs)\n",
    "    print(\"Done\\n\")\n",
    "\n",
    "    index_name = input(\"|***| Enter the name of name of Pinecone index that you want to create or fetch from.\")\n",
    "    print(\"---- Upload Chunks to Vector Store\\n\")\n",
    "    if embeddings_type == 'openai': print(\"Estimated cost:\", print_embedding_cost(chunks))\n",
    "    vector_store = insert_or_fetch_embeddings(index_name= index_name, chunks= chunks, embeddings_type=embeddings_type)\n",
    "    print(\"------- Phase 2: Setting up knowledge based vector store complete -------\\n\")\n",
    "\n",
    "\n",
    "    # 4. Build LLM Chain\n",
    "    llmchain = get_llm_chain(vector_store=vector_store, llm_type = llm_type, has_memory=has_memory,)\n",
    "\n",
    "    #5. Asking Questions & Getting Answers\n",
    "    import time\n",
    "    num = 1\n",
    "    print(\"*\"*77)\n",
    "    print('****************** Your private chatbot is ready for use! ******************')\n",
    "    print(\"*\"*77+\"\\nEnter Q/ Quit/ Exit to quit.\")\n",
    "    while True:\n",
    "        print(f'{\"-\" * 50} \\n')\n",
    "        q = input(f'Question #{num}: ')\n",
    "        \n",
    "        if q.lower() in ['q', 'quit', 'exit']:\n",
    "            print('Conversation ended...')\n",
    "            break\n",
    "\n",
    "        answer, chat_history = ask(query=q, llm_chain=llmchain, has_memory=has_memory, chat_history = [])\n",
    "        print(f'Answer: {answer}')\n",
    "        if has_memory:\n",
    "            print(f\"Chat History: {chat_history}\")\n",
    "        num += 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e83c64",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------- \n",
      "--                                                                  -- \n",
      "--            Welcome to your private chat application. !           -- \n",
      "--                                                                  --\n",
      "----------------------------------------------------------------------\n",
      "Before you start using your private chatbot, please complete Phase 1 & Phase 2 setiup.!\n",
      "------- Phase 1: Configure your chat agent \n",
      "\n",
      "|***| Select the LLM you want to use? Enter the corrresponding number or Press enter to use the deault one\n",
      "\t1. Google/flan-t5-xxl (default)\n",
      "\t2. OpenAI\n",
      "1\n",
      "\tSelected Google/flan-t5-xxl (default)\n",
      "\n",
      "|***| Select the embeddings you want to use? Enter the corrresponding number or Press enter to use the deault one\n",
      "\t1. instructor-xl (default)\n",
      "\t2. OpenAI\n",
      "1\n",
      "\tSelected instructor-xl (default)\n",
      "\n",
      "|***| Do you want your agent to have memory and remember your older conversations?\n",
      "\t1. No (default)\n",
      "\t2. Yes\n",
      "2\n",
      "\tSelected Yes\n",
      "------- Phase 1: Configuration complete -------\n",
      "\n",
      "\n",
      "------- Phase 2: Build Knowledge Base -------\n",
      "\n",
      "|***| Select where should the LLM build the knowledge base from? Enter the corrresponding number or Press enter to use the deault one\n",
      "\t1. Local files (Supported formats: '.pdf', '.txt', '.md' (default)\n",
      "\t2. Web (Wikipedia)\n",
      "1\n",
      "\tSelected Local files\n",
      "\n",
      "Enter the path (file/ folder) to read documents from. (Supported formats: '.pdf', '.txt', '.md')../../../Code\n",
      "Reading from folder ../../../Code\n",
      "Unsupported File Format ../../../Code/.DS_Store. Supported formats: .pdf, .docx, .txt, .md\n",
      "Unsupported File Format ../../../Code/Udemy Course Resources-20230907T070241Z-001.zip. Supported formats: .pdf, .docx, .txt, .md\n",
      "Reading ../../../Code/HP1 - Harry Potter and the Sorcerer's Stone.pdf\n",
      "\n",
      "---- Chunking the Data ... Done\n",
      "\n",
      "|***| Enter the name of name of Pinecone index that you want to create or fetch from.harry-potter\n",
      "---- Upload Chunks to Vector Store\n",
      "\n",
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n",
      "Index harry-potter already exists. Loading embeddings ... Done\n",
      "------- Phase 2: Setting up knowledge based vector store complete -------\n",
      "\n",
      "*****************************************************************************\n",
      "****************** Your private chatbot is ready for use! ******************\n",
      "*****************************************************************************\n",
      "Enter Q/ Quit/ Exit to quit.\n",
      "-------------------------------------------------- \n",
      "\n",
      "Question #1: Where was Harry before going to Hogwarts?\n",
      "Answer: Privet Drive\n",
      "Chat History: [HumanMessage(content='Where was Harry before going to Hogwarts?', additional_kwargs={}, example=False), AIMessage(content='Privet Drive', additional_kwargs={}, example=False)]\n",
      "-------------------------------------------------- \n",
      "\n",
      "Question #2: Who is the principal of Hogwarts?\n",
      "Answer: Albus Dumbledore\n",
      "Chat History: [HumanMessage(content='Where was Harry before going to Hogwarts?', additional_kwargs={}, example=False), AIMessage(content='Privet Drive', additional_kwargs={}, example=False), HumanMessage(content='Who is the principal of Hogwarts?', additional_kwargs={}, example=False), AIMessage(content='Albus Dumbledore', additional_kwargs={}, example=False)]\n",
      "-------------------------------------------------- \n",
      "\n",
      "Question #3: What are the 4 houses in Hogwarts?\n",
      "Answer: G ryffindor, Hufflepuff, Ravenclaw, and Slyth\n",
      "Chat History: [HumanMessage(content='Where was Harry before going to Hogwarts?', additional_kwargs={}, example=False), AIMessage(content='Privet Drive', additional_kwargs={}, example=False), HumanMessage(content='Who is the principal of Hogwarts?', additional_kwargs={}, example=False), AIMessage(content='Albus Dumbledore', additional_kwargs={}, example=False), HumanMessage(content='What are the 4 houses in Hogwarts?', additional_kwargs={}, example=False), AIMessage(content='G ryffindor, Hufflepuff, Ravenclaw, and Slyth', additional_kwargs={}, example=False)]\n",
      "-------------------------------------------------- \n",
      "\n",
      "Question #4: Who are the professors at Hogwarts mentioned in the book?\n",
      "Answer: Professor Spro ut — Professor Flitwick — Professor McGonagall \n",
      "Chat History: [HumanMessage(content='Where was Harry before going to Hogwarts?', additional_kwargs={}, example=False), AIMessage(content='Privet Drive', additional_kwargs={}, example=False), HumanMessage(content='Who is the principal of Hogwarts?', additional_kwargs={}, example=False), AIMessage(content='Albus Dumbledore', additional_kwargs={}, example=False), HumanMessage(content='What are the 4 houses in Hogwarts?', additional_kwargs={}, example=False), AIMessage(content='G ryffindor, Hufflepuff, Ravenclaw, and Slyth', additional_kwargs={}, example=False), HumanMessage(content='Who are the professors at Hogwarts mentioned in the book?', additional_kwargs={}, example=False), AIMessage(content='Professor Spro ut — Professor Flitwick — Professor McGonagall ', additional_kwargs={}, example=False)]\n",
      "-------------------------------------------------- \n",
      "\n",
      "Question #5: Who was playing the first Quidditch match in the book?\n",
      "Answer: Gryffindor versus Slytherin\n",
      "Chat History: [HumanMessage(content='Where was Harry before going to Hogwarts?', additional_kwargs={}, example=False), AIMessage(content='Privet Drive', additional_kwargs={}, example=False), HumanMessage(content='Who is the principal of Hogwarts?', additional_kwargs={}, example=False), AIMessage(content='Albus Dumbledore', additional_kwargs={}, example=False), HumanMessage(content='What are the 4 houses in Hogwarts?', additional_kwargs={}, example=False), AIMessage(content='G ryffindor, Hufflepuff, Ravenclaw, and Slyth', additional_kwargs={}, example=False), HumanMessage(content='Who are the professors at Hogwarts mentioned in the book?', additional_kwargs={}, example=False), AIMessage(content='Professor Spro ut — Professor Flitwick — Professor McGonagall ', additional_kwargs={}, example=False), HumanMessage(content='Who was playing the first Quidditch match in the book?', additional_kwargs={}, example=False), AIMessage(content='Gryffindor versus Slytherin', additional_kwargs={}, example=False)]\n",
      "-------------------------------------------------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd3942b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
